{
    "generate_text": [
        {
            "title": "Performance Approaches to Semantics in Human Language",
            "tl;dr": "",         
            "abstract": "Over the course of human history, prominent hypotheses for the source of human behav-ioral uniqueness have included freedom from metaphysical necessity, rational intention, andstrong social altruism. However, when applied to questions concerning the evolution andcurrent use of human language, these hypotheses generate more problems than they solve.These include everything from the statistical improbability of hopeful monster mutationsto the unstable strategy of altruism without reciprocity. Adopting a performative view ofhuman language removes these problematic dependencies, and explains aspects of languagelike phatic speech, fossils, and register, that require special pleading by one or more of thepopular hypotheses.This thesis provides two pieces of computational evidence for a performative view of hu-man language. First, it shows that the possibility space of human language is much smallerthan expected for a rule based but otherwise semantically unrestricted communication sys-tem. Second, it shows that the semantic information contained in an entire speech act ispoorly predicted by the semantic information of its individual parts, as might be expectedfor a compositional system designed for efficient communication.To conduct this second analysis, a novel method was developed based on the distribu-tional theory of semantics for measuring the information content of human speech. Brie y,it is a measure of the distance between the distribution of terms in a language and the condi-tional probability of their appearance within a context of interest. This same method showspromise for testing other hypotheses about the nature and origin of language that involvean informational component.",
            "keywords": "",   
            "section": "outline",
            "input": "",         
            "output": ""    
        }
        
    ],
    "generate_abstract": [
        {
            "title": "Performance Approaches to Semantics in Human Language",
            "tl;dr": "",         
            "abstract": "Over the course of human history, prominent hypotheses for the source of human behav-ioral uniqueness have included freedom from metaphysical necessity, rational intention, andstrong social altruism. However, when applied to questions concerning the evolution andcurrent use of human language, these hypotheses generate more problems than they solve.These include everything from the statistical improbability of hopeful monster mutationsto the unstable strategy of altruism without reciprocity. Adopting a performative view ofhuman language removes these problematic dependencies, and explains aspects of languagelike phatic speech, fossils, and register, that require special pleading by one or more of thepopular hypotheses.This thesis provides two pieces of computational evidence for a performative view of hu-man language. First, it shows that the possibility space of human language is much smallerthan expected for a rule based but otherwise semantically unrestricted communication sys-tem. Second, it shows that the semantic information contained in an entire speech act ispoorly predicted by the semantic information of its individual parts, as might be expectedfor a compositional system designed for efficient communication.To conduct this second analysis, a novel method was developed based on the distribu-tional theory of semantics for measuring the information content of human speech. Brie y,it is a measure of the distance between the distribution of terms in a language and the condi-tional probability of their appearance within a context of interest. This same method showspromise for testing other hypotheses about the nature and origin of language that involvean informational component.",
            "keywords": "",   
            "section": "abstract",
            "input": "",         
            "output": "Over the course of human history, prominent hypotheses for the source of human behav-ioral uniqueness have included freedom from metaphysical necessity, rational intention, andstrong social altruism. However, when applied to questions concerning the evolution andcurrent use of human language, these hypotheses generate more problems than they solve.These include everything from the statistical improbability of hopeful monster mutationsto the unstable strategy of altruism without reciprocity. Adopting a performative view ofhuman language removes these problematic dependencies, and explains aspects of languagelike phatic speech, fossils, and register, that require special pleading by one or more of thepopular hypotheses.This thesis provides two pieces of computational evidence for a performative view of hu-man language. First, it shows that the possibility space of human language is much smallerthan expected for a rule based but otherwise semantically unrestricted communication sys-tem. Second, it shows that the semantic information contained in an entire speech act ispoorly predicted by the semantic information of its individual parts, as might be expectedfor a compositional system designed for efficient communication.To conduct this second analysis, a novel method was developed based on the distribu-tional theory of semantics for measuring the information content of human speech. Brie y,it is a measure of the distance between the distribution of terms in a language and the condi-tional probability of their appearance within a context of interest. This same method showspromise for testing other hypotheses about the nature and origin of language that involvean informational component."    
        }
        
    ],
    "generate_intro": [
        {
            "title": "Performance Approaches to Semantics in Human Language",
            "tl;dr": "",         
            "abstract": "Over the course of human history, prominent hypotheses for the source of human behav-ioral uniqueness have included freedom from metaphysical necessity, rational intention, andstrong social altruism. However, when applied to questions concerning the evolution andcurrent use of human language, these hypotheses generate more problems than they solve.These include everything from the statistical improbability of hopeful monster mutationsto the unstable strategy of altruism without reciprocity. Adopting a performative view ofhuman language removes these problematic dependencies, and explains aspects of languagelike phatic speech, fossils, and register, that require special pleading by one or more of thepopular hypotheses.This thesis provides two pieces of computational evidence for a performative view of hu-man language. First, it shows that the possibility space of human language is much smallerthan expected for a rule based but otherwise semantically unrestricted communication sys-tem. Second, it shows that the semantic information contained in an entire speech act ispoorly predicted by the semantic information of its individual parts, as might be expectedfor a compositional system designed for efficient communication.To conduct this second analysis, a novel method was developed based on the distribu-tional theory of semantics for measuring the information content of human speech. Brie y,it is a measure of the distance between the distribution of terms in a language and the condi-tional probability of their appearance within a context of interest. This same method showspromise for testing other hypotheses about the nature and origin of language that involvean informational component.",
            "keywords": "",   
            "section": "introduction",
            "input": "",         
            "output": "It is an often repeated assertion that human languages are unbounded in their generativity.A recent search on Google Scholar1for example, returned over 2000 results for academicpapers containing the exact phrase in nite number of sentences. One imagines that thetotal number of papers containing a similar sentiment is substantially larger.  The claimof in nite generativity relies on the metaphor that a human brain operates much like acomputer. Chomsky, for example, writeslanguage, is, at its core, a system that is both digital and in nite, and works bythe in nite use of  nite means.[chomsky_1991]One form of reasoning behind this claim relies on some combination of an open sys-tem of reference tied to a recursive syntax.  In a fully recursive language, there are nobounds on how long sentences can become, or how deeply their elements can be nested[hauser_chomsky_ tch_2002].  In this case, the total number of possible sentences ableto be constructed is discrete but also unlimited, and therefore in nite. An open system ofreference means that it is always possible to add new symbols that refer to referents or evenother symbols [deacon_1997].Another form is to keep the underlying mechanism as a black box, but to still assume thathumans are using language to disambiguate signals in their environment. Randall Munroe,author of the popular comic stripxkcd, recently used Claude Shannon's estimate of theentropy of English characters to estimate the total number of possible English tweets (seeEq.5.1) shannon_1948. He arrives at the conclusion that there are  a numberlarge enough2to be considered in nite in practicality, even if not in actuality munroe_2013. To be sure, this kind of random process is not used to model language in practice, but doesprovide a reasonable estimate for the size of language under a generative model, especiallygiven the lack of alternative predictions to test. The in nite generativity argument is often raised against children learning to speak justby example. The reasoning here is that any size exposure to an in nite system comprises0% of its total variance, and it is therefore impossible to make correct inferences about therules governing the creation of that language. In other words, because the amount of globalvariation is so large compared to the possible size of local exposure, humans must have somesort of prior knowledge about the structure of the language to bootstrap their acquisition.Evidence that young learners hear a larger fraction of their  rst language would remove someof the motivations behind innate language arguments.However, both of these approaches assume that human language samples its semanticpossibility space rectangularly, when we know that humans make preferential use of somecombinations of their language [zipf_1945,weinert_1995]. There is the equally troublesomeobservation that humans do not use all or even most of the syntactically correct ways toexpress an idea, but prefer to stick with one standard means of expression [hymes_1970,bygate_1988]. This holds true even when that favored expression does not conform to theagreed syntactic rules [pawley_syder_1983,wray_1998].Munroe's estimate might be a good approximation of the true number of unique En-glish tweets if humans decide what to tweet by randomly sampling the space of all possibletweets. This seems unlikely for a few reasons. On a super cial level, most tweets are goingto be about temporally relevant things, often constrained by conversations and audience[marwick_boyd_2010]. Thinking a little deeper, the possibility space of language only re- ects combinations of things within the lived experience of humans3. Finally, at the mostprofound level, the possibility space of human language at timetmust be limited to one oronly a few steps beyond the space att1 because of the way that cultural knowledge istransmitted and transformed.Additionally, there is a large cognitive constraint in terms of the size and recursive depthof linguistics constructs and their capacity to overload human memory.  This is usuallynoted but also usually ignored, following an old argument about the importance of studyingcompetence versus studying performance4[chomsky_1965]. We do not plan on taking upthis particular debate here, as there is not room enough to adequately cover the evolvingarguments on both sides. Outside of the context of that debate, it is important to bear inmind that there is a real physical limitation on the length and depth of human language which practically restricts recursive depth and therefore also the reality-constrained possi-bility space.To assess the probability that human language is practically in nite, we employed anempirical method to estimate the function governing the probability of encountering a newsentence after observing a sample ofnsentences.  This method was applied to a corpusof conversational, text-based data collected online. We used this function to calculate thesample size at which hearing a new sentence becomes statistically improbable6, which wetake as our estimate for the total number of unique sentences. We then use this function tomake estimates for the timing of some landmarks in language acquisition."
        }
 
    ],
    "generate_survey": [
        {
            "title": "Performance Approaches to Semantics in Human Language",
            "tl;dr": "",         
            "abstract": "Over the course of human history, prominent hypotheses for the source of human behav-ioral uniqueness have included freedom from metaphysical necessity, rational intention, andstrong social altruism. However, when applied to questions concerning the evolution andcurrent use of human language, these hypotheses generate more problems than they solve.These include everything from the statistical improbability of hopeful monster mutationsto the unstable strategy of altruism without reciprocity. Adopting a performative view ofhuman language removes these problematic dependencies, and explains aspects of languagelike phatic speech, fossils, and register, that require special pleading by one or more of thepopular hypotheses.This thesis provides two pieces of computational evidence for a performative view of hu-man language. First, it shows that the possibility space of human language is much smallerthan expected for a rule based but otherwise semantically unrestricted communication sys-tem. Second, it shows that the semantic information contained in an entire speech act ispoorly predicted by the semantic information of its individual parts, as might be expectedfor a compositional system designed for efficient communication.To conduct this second analysis, a novel method was developed based on the distribu-tional theory of semantics for measuring the information content of human speech. Brie y,it is a measure of the distance between the distribution of terms in a language and the condi-tional probability of their appearance within a context of interest. This same method showspromise for testing other hypotheses about the nature and origin of language that involvean informational component.",
            "keywords": "",   
            "section": "survey",
            "input": "",         
            "output": ""     
        }
       
    ],
    "generate_methodology": [
        {
            "title": "Performance Approaches to Semantics in Human Language",
            "tl;dr": "",         
            "abstract": "Over the course of human history, prominent hypotheses for the source of human behav-ioral uniqueness have included freedom from metaphysical necessity, rational intention, andstrong social altruism. However, when applied to questions concerning the evolution andcurrent use of human language, these hypotheses generate more problems than they solve.These include everything from the statistical improbability of hopeful monster mutationsto the unstable strategy of altruism without reciprocity. Adopting a performative view ofhuman language removes these problematic dependencies, and explains aspects of languagelike phatic speech, fossils, and register, that require special pleading by one or more of thepopular hypotheses.This thesis provides two pieces of computational evidence for a performative view of hu-man language. First, it shows that the possibility space of human language is much smallerthan expected for a rule based but otherwise semantically unrestricted communication sys-tem. Second, it shows that the semantic information contained in an entire speech act ispoorly predicted by the semantic information of its individual parts, as might be expectedfor a compositional system designed for efficient communication.To conduct this second analysis, a novel method was developed based on the distribu-tional theory of semantics for measuring the information content of human speech. Brie y,it is a measure of the distance between the distribution of terms in a language and the condi-tional probability of their appearance within a context of interest. This same method showspromise for testing other hypotheses about the nature and origin of language that involvean informational component.",
            "keywords": "",   
            "section": "methodology",
            "input": "",         
            "output": "ProcessingData were collected in Python 3.5.1 on Ubuntu Server 14.0.4, and were analyzed with Mi-crosoft R Open7based on CRAN release v. 3.2.3, Wooden Christmas-Tree, employingtidyr and dplyr for data reshaping [r,tidyr,dplyr]. Tables were produced with xtable, and gures were produced with ggplot2 [xtable,ggplot2]. The python program for generatingthe data, the raw data, and the R markdown  le necessary to reproduce this analysis alongwith its tables and  gures are available atgithub.com/deniederhut[knitr].It is desirable to conduct analyses on corpora that are both large and ecologically valid.However, there is an inherent trade-off between these two features, as the closer one getsto spontaneous conversation, the harder the data become to transform and process. As anexample, the largest readily available corpus, Google Books, is constructed from the scannedtext of library collections. As a result, it tends to have a larger vocabulary, fewer idiomsand slang words, more standard grammar, and more obscure and technical topics than theEnglish employed by contemporary speakers [pechenik_danforth_dodds_2015].To strike a balance between validity and size, a sample of 8Mcomments of conversationalEnglish was collected from the online discussion forum,AskReddit. These comments wereseparated into individual sentences using NLTK's sentence tokenizer [nltk], resulting in asample size of 15Msentences and 250Mwords, making it somewhat larger than the Browncorpus at  1Mwords. These sentences had an average length of 77:8 characters, and 17:1words, based on applying NLTK's word tokenizer to the corpus.This population of sentences was sampled without replacement by randomly drawing sizenof sentences, then iteratively downsampling it byn2until the sample size was less than10. At each iteration, three methods of determining the uniqueness of a sentence were usedto compare each sentence in the subsample with the rest, and the total number of uniquesentences, the method of calculation, and the size of the sample were recorded. Becausethese three methods differed greatly in computational complexity, the slower methods were not applied to sample sizes that would have taken longer than 120 hours wall clock time tocompute.Similarity metricsThe fastest of these three methods was to use Python's built-insetconstructor, whichde nes a duplicate sentence as one in which every byte matches. In other words, this metricconsidersThis is my houseandThis is my  houseto be two unique sentences. This method is desirable for running in linear time, but unde-sirable in being very conservative. Below, we will refer to this asbitwisesimilarity. Bitwisesimilarity was applied to all samples, including the entire corpus of 15Msentences.One potential way to avoid classifying two sentences as unique when they differ only bywhitespace is to use a similarity metric for the character strings. Our second test method usesfuzzywuzzy's implementation of Levenshtein distance to judge the similarity of two stringsbased on fuzzy matching of the character combinations [fuzzywuzzy]. Below, we refer tothis asLevenshteinsimilarity. Because Levenshtein similarity was quite a bit slower thanbitwise similarity, it was not used on samples>= 150;000 sentences.The third method uses gensim's implementation of cosine similarity to judge the seman-tic similarity of two sentences. In this method, each pair of sentences was tokenized, andvectors were built fromidfweighting them against a tertiary document containing the 25most common English unigrams, in their relative ecological proportions. Then, the cosinesimilarity of the two sentence vectors was calculated. This was our current best approxi-mation of our intuition about what makes a sentence unique, but it may still overestimatethe difference between two sentences. Below, we refer to this ascosinesimilarity. Becausecosine similarity was the slowest of all the test metrics, it was only applied to relatively small(<= 23;000) samples.These latter two methods had their parameters tuned such thatThis isn't my houseandThis is not my housewere judged to be the same sentence, andThis is my house andThis is not my housewere judged to be different sentences. This was achieved by setting the match strength to90% and 50% for Levenshtein distance and cosine similarity, respectively.ModelTo estimate when a person will stop encountering new sentences as a function of the numberof sentences already observed, we must  rst estimate the probability function governingthe appearance of new sentences. We chose to model this function as the log probabilityof drawing a new sentence from a population after samplingnsentences. The proportionof unique sentences at each sample size was calculated by simply dividing the number ofunique sentences from each method by the sample size from that iteration. To coerce therelationship between proportion unique and sample size into the characteristicSshape of alogit curve, the sample sizes were log transformed When the logit of the proportion is taken, this relationship linearizes (Fig.5.2). Data fromeach of the similarity methods were  tted with a linear model predicting logit proportionfrom log sample size.These models were then used to produce estimates of the number of unique sentencesobserved by major developmental landmarks, including time to  rst word,  rst sentence,and  rst story, which occur around the ages of 1, 3, and 5, respectively. These were thendivided by the number of unique sentences necessary to have been observed to produce a 1%chance or less that the next sentence encountered would be novel. The values produced bythis method were compared to the proportion of sentences observed using the informationtheoretic measure, multiplied by 99% of its total size to keep the scaling consistent."
        }
        
    ],
    "generate_experiment": [
        {
            "title": "Performance Approaches to Semantics in Human Language",
            "tl;dr": "",         
            "abstract": "Over the course of human history, prominent hypotheses for the source of human behav-ioral uniqueness have included freedom from metaphysical necessity, rational intention, andstrong social altruism. However, when applied to questions concerning the evolution andcurrent use of human language, these hypotheses generate more problems than they solve.These include everything from the statistical improbability of hopeful monster mutationsto the unstable strategy of altruism without reciprocity. Adopting a performative view ofhuman language removes these problematic dependencies, and explains aspects of languagelike phatic speech, fossils, and register, that require special pleading by one or more of thepopular hypotheses.This thesis provides two pieces of computational evidence for a performative view of hu-man language. First, it shows that the possibility space of human language is much smallerthan expected for a rule based but otherwise semantically unrestricted communication sys-tem. Second, it shows that the semantic information contained in an entire speech act ispoorly predicted by the semantic information of its individual parts, as might be expectedfor a compositional system designed for efficient communication.To conduct this second analysis, a novel method was developed based on the distribu-tional theory of semantics for measuring the information content of human speech. Brie y,it is a measure of the distance between the distribution of terms in a language and the condi-tional probability of their appearance within a context of interest. This same method showspromise for testing other hypotheses about the nature and origin of language that involvean informational component.",
            "keywords": "",   
            "section": "experiment",
            "input": "",         
            "output": ""  
        }
        
    ],
    "generate_discussion": [
        {
            "title": "Performance Approaches to Semantics in Human Language",
            "tl;dr": "",         
            "abstract": "Over the course of human history, prominent hypotheses for the source of human behav-ioral uniqueness have included freedom from metaphysical necessity, rational intention, andstrong social altruism. However, when applied to questions concerning the evolution andcurrent use of human language, these hypotheses generate more problems than they solve.These include everything from the statistical improbability of hopeful monster mutationsto the unstable strategy of altruism without reciprocity. Adopting a performative view ofhuman language removes these problematic dependencies, and explains aspects of languagelike phatic speech, fossils, and register, that require special pleading by one or more of thepopular hypotheses.This thesis provides two pieces of computational evidence for a performative view of hu-man language. First, it shows that the possibility space of human language is much smallerthan expected for a rule based but otherwise semantically unrestricted communication sys-tem. Second, it shows that the semantic information contained in an entire speech act ispoorly predicted by the semantic information of its individual parts, as might be expectedfor a compositional system designed for efficient communication.To conduct this second analysis, a novel method was developed based on the distribu-tional theory of semantics for measuring the information content of human speech. Brie y,it is a measure of the distance between the distribution of terms in a language and the condi-tional probability of their appearance within a context of interest. This same method showspromise for testing other hypotheses about the nature and origin of language that involvean informational component.",
            "keywords": "",   
            "section": "discussion",
            "input": "",         
            "output": "While the total theoretical number of unique sentences in English is very large, the numberthat are actually used in conversation is smaller by several orders of magnitude. Using theShannon method for this corpus, we can use our measurement of average sentence length, 78characters, to get an estimate for the theoretical number of English sentences,(seeEq.5.1). While slightly smaller, this is not appreciably different than Munroe's estimate fortweets.However, our estimates for the number of probable unique sentences using the bitwisemethod, is more than ten orders of magnitude smaller8(Tab.5.4). We consider a difference of fourteen orders of magnitude between the empirical estimate and thetheoretical estimate to indicate that the theoretical one may be unrealistic.To be sure,  ve hundred billion unique English sentences is still a very large number{ much larger than any person will encounter in their lifetime. However, there is no needto hear every possible sentence in order to infer enough about the structure of a languageto start producing your own sentences in it.  It should be possible to do so based on arelatively small sample, especially when that language has regular grammar rules that applyto the more uncommon utterances; and, when many of the sentences heard in daily life areritualized utterances surrounding typical social situations.Let's assume for the sake of argument that the average person hears about nine hun-dred sentences per day, or per year (a rough estimate based on research from[mehl_et_al_2007]). We can use this value to do a back of the envelope calculation of theprobability of encountering a new sentence by age.Using just this estimate for number of sentences per year, we can apply the logit modelin a fairly straightforward manner to calculate that a 7 year old child can expect only 90%of the sentences they hear to be novel9. By their 60s, a person has only an 77% chance ofhearing something new.We can also use this model to ask a slightly different question that will allow us to makedirect comparisons with the Shannon estimates { what is the fraction of all possible sentencesa person can expect to have heard at any age? This is computationally trickier, but a fewkey milestones have been been plotted for both the theoretical estimate and our ecologicalestimates (Fig.5.4).It is clear here that a poverty of the stimulus type argument clearly applies for theShannon estimates, as the proportion of language experienced by a human during their liferemains 0%. The bitwise and cosine estimates, however, grow visibly larger over the course of the lifespan, until a human can expect to have experienced 1=300th of the semantically(cosine) unique possibilities of their language by the end of their life. This seems to be amore realistic estimate on its face, and paints a different picture of the relative quantity oflanguage received by humans while learning to speak.One interesting consequence of calculating time-to-competence this way is that the rateof exposure to English matters a great deal. It's possible that the well-documented effectof reading on increasing language  uency is simply a side effect of the fact that it is fasterto read words, roughly 5=3 as fast in English, than to listen to them spoken aloud10. Ifa person were to replace all of the time they spent listening with time spent reading, theproportion of the total possible sentences in English they would be exposed to would roughlydouble over the course of their life, with their likelihood of surprisal would only drop from77% to 72%."
        }
    ],
    "generate_conclusion": [
        {
            "title": "Performance Approaches to Semantics in Human Language",
            "tl;dr": "",         
            "abstract": "Over the course of human history, prominent hypotheses for the source of human behav-ioral uniqueness have included freedom from metaphysical necessity, rational intention, andstrong social altruism. However, when applied to questions concerning the evolution andcurrent use of human language, these hypotheses generate more problems than they solve.These include everything from the statistical improbability of hopeful monster mutationsto the unstable strategy of altruism without reciprocity. Adopting a performative view ofhuman language removes these problematic dependencies, and explains aspects of languagelike phatic speech, fossils, and register, that require special pleading by one or more of thepopular hypotheses.This thesis provides two pieces of computational evidence for a performative view of hu-man language. First, it shows that the possibility space of human language is much smallerthan expected for a rule based but otherwise semantically unrestricted communication sys-tem. Second, it shows that the semantic information contained in an entire speech act ispoorly predicted by the semantic information of its individual parts, as might be expectedfor a compositional system designed for efficient communication.To conduct this second analysis, a novel method was developed based on the distribu-tional theory of semantics for measuring the information content of human speech. Brie y,it is a measure of the distance between the distribution of terms in a language and the condi-tional probability of their appearance within a context of interest. This same method showspromise for testing other hypotheses about the nature and origin of language that involvean informational component.",
            "keywords": "",   
            "section": "conclusion",
            "input": "",         
            "output": "There are some concerns with the estimates presented here.  First, the English corpus isdrawn from a particular sociocultural community which largely consists of 18-35 year oldeducated white males living in the United States, and thus fails to capture the full diversityof the English language. Additionally, the corpus was collected over a relatively short periodof time { just 6 months. An actual human is exposed to language that changes over time,presenting something of a moving target. Finally, the amount of exposure is only one factorin learning a language, which must also include factors like pragmatic usage and socialcontext.However, our estimates for the number of likely sentences in English bene t from beinggrounded in observations of real use of language. They are generated from a probabilisticmodel of encountering sentences, which is more ecologically valid than a model which treatsthe distribution of sentences to be rectangular in their probability of appearance. Finally,they also produce more reasonable values for language exposure.These estimates were used to produce three major sets of  ndings. First, that a largeportion of any person's language exposure is to the same set of common sentences.  Wesaw this above where an adult in their 80s has a 23% chance that the next sentence theyhear will be something they have heard before. Halving this probability of surprisal dropsthe requisite age by a factor of seven11. Second, we deduce that the total number of likelysentences is much smaller using empirical estimation, suggesting that theoretical estimatesgreatly overestimate the number of viable sentences in a language. Third, the models  ttedto empirical data produce a more reasonable account of how much language a child needs tohear to make inferences about the structure of that language."    
        }

    ]
    
} 